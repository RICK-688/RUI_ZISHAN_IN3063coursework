{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cdffc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b57cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the activation functions and their back propagation\n",
    "# No need to preserve parameters for these calculations can be done based on these values\n",
    "def sigmoid(x):\n",
    "    return np.array([1/(1+np.exp(-x.squeeze()))])\n",
    "\n",
    "def relu(x):\n",
    "    return np.array([np.maximum(0, x.squeeze())])\n",
    "\n",
    "# The shape of the previous gradient shall be [1, n]\n",
    "def sigmoid_backward(prev_grad, cache):\n",
    "    (x, w, b) = cache\n",
    "    temp = np.dot(x, w)+b\n",
    "    return np.array([prev_grad.squeeze()*sigmoid(temp).squeeze()*(1-sigmoid(temp).squeeze())])\n",
    "\n",
    "def relu_backward(prev_grad, cache):\n",
    "    (x, w, b) = cache\n",
    "    temp = np.dot(x, w)+b\n",
    "    output = []\n",
    "    for i in range(len(temp.squeeze())):\n",
    "        if temp.squeeze()[i] > 0:\n",
    "            output.append(prev_grad.squeeze()[i])\n",
    "        else:output.append(0)\n",
    "    return np.array([output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bcebda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a softmax layer\n",
    "def softmax(x):\n",
    "    x = x.squeeze().astype(float)\n",
    "    return np.array([np.exp(x)/sum(np.exp(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba8ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single layer and its back propagation\n",
    "# Shapes: x-[1, n] w-[n, m] b-[1, m]\n",
    "def single_layer(x, w, b):\n",
    "    # This is used to store the parameters for back propagation\n",
    "    cache = (x, w, b)\n",
    "    return cache, np.dot(x, w)+b\n",
    "\n",
    "# The shape of the previous gradient shall be [1, m]\n",
    "def single_layer_backward(prev_grad, cache):\n",
    "    (x, w, b) = cache\n",
    "    # Here we shall calculate the gradient of weight and bias \n",
    "    grad_w = np.dot(x.transpose(), prev_grad)\n",
    "    grad_b = np.sum(prev_grad, keepdims=True)\n",
    "    current_grad = np.dot(prev_grad, w.transpose())\n",
    "    grad = (grad_w, grad_b)\n",
    "    return current_grad, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8848cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shall initialize the parameters based on the structure\n",
    "def initializer(structure):\n",
    "    weight = []\n",
    "    bias = []\n",
    "    \n",
    "    # The total layer shall be n-1, where n denotes the dimension of the structure\n",
    "    for i in range(len(structure)-1):\n",
    "        w = np.random.rand(structure[i], structure[i+1])/50\n",
    "        b = np.random.rand(1, structure[i+1])/1000\n",
    "        weight.append(w)\n",
    "        bias.append(b)\n",
    "    return weight, bias, len(structure)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e120ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, y):\n",
    "    return -sum(y*np.log(pred.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af4bf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward and the backward propagation of the network\n",
    "def neural_network(x, y, layers, weight, bias, activation_type=\"relu\", requires_cache=True):\n",
    "    # This is used to store the parameters\n",
    "    # When training, requires_cache is set to true\n",
    "    caches = []\n",
    "    temp = np.array(x, copy=True)\n",
    "    for i in range(layers-1):\n",
    "        cache, temp = single_layer(temp, weight[i], bias[i])\n",
    "        if requires_cache:\n",
    "            caches.append(cache)\n",
    "        \n",
    "        # Apply the activation function\n",
    "        if activation_type == \"relu\":\n",
    "            temp = relu(temp)\n",
    "        elif activation_type == \"sigmoid\":\n",
    "            temp = sigmoid(temp)\n",
    "        else: raise ValueError(\"No valid activation type\")\n",
    "    \n",
    "    # The last layer is connected with softmax function\n",
    "    cache, temp = single_layer(temp, weight[-1], bias[-1])\n",
    "    if requires_cache:\n",
    "        caches.append(cache)\n",
    "    output = softmax(temp)\n",
    "    if requires_cache:\n",
    "        caches.append((output, y))\n",
    "    \n",
    "    # Use output to calculate the loss value\n",
    "    loss = cross_entropy(output, y)\n",
    "    \n",
    "    # Return the predictions, the loss value and the caches\n",
    "    return output, loss, caches\n",
    "\n",
    "# The backward propagation of the network\n",
    "# Caches store the variables in each layer and have the form of (x, w, b), despite of the last layer\n",
    "def neural_network_backward(layers, caches, activation_type=\"relu\"):\n",
    "    # Use the last layer in the caches to initialize the gradient\n",
    "    grads = []\n",
    "    cache = caches[-1]\n",
    "    (output, y) = cache\n",
    "    current_grad = np.array([output.squeeze()-y])\n",
    "    current_grad, grad = single_layer_backward(current_grad, caches[-2])\n",
    "    grads.append(grad)\n",
    "    \n",
    "    # Deal with the rest of the layers\n",
    "    for i in range(layers-1):\n",
    "        cache = caches[-i-3]\n",
    "        \n",
    "        if activation_type == \"relu\":\n",
    "            current_grad = relu_backward(current_grad, cache)\n",
    "        elif activation_type == \"sigmoid\":\n",
    "            current_grad = sigmoid_backward(current_grad, cache)\n",
    "        else: raise ValueError(\"No valid activation type\")\n",
    "            \n",
    "        # Backprop in layers\n",
    "        current_grad, grad = single_layer_backward(current_grad, cache)\n",
    "        \n",
    "        # Save the gradient\n",
    "        grads.append(grad)\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b5d5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a network\n",
    "def train(x, y, structure, epochs=10, learning_rate=0.00001, activation_type=\"relu\", sample_number=5000):\n",
    "    # Initialize the network\n",
    "    weight, bias, layers = initializer(structure)\n",
    "    least_avg_loss = 10000000\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # For SGD, we randomly pick 10 samples to update the parameters\n",
    "        indices = np.random.choice(len(x), sample_number, replace=False)\n",
    "        average_loss = 0\n",
    "        weight_update = []\n",
    "        bias_update = []\n",
    "        # Forward propagation, only recieve the caches of the picked samples\n",
    "        for i in range(len(x)):\n",
    "            if i in indices:\n",
    "                requires_cache = True\n",
    "            else: requires_cache = False\n",
    "\n",
    "            _, loss, caches = neural_network(x[i], y[i], layers, weight, bias, activation_type, requires_cache)\n",
    "            \n",
    "            average_loss += loss/len(x)\n",
    "            # Back propagation only when caches are received\n",
    "            if requires_cache:\n",
    "                grads = neural_network_backward(layers, caches, activation_type)\n",
    "                \n",
    "                # Update the gradients\n",
    "                if len(weight_update) == 0:\n",
    "                    for grad in grads:\n",
    "                        (grad_w, grad_b) = grad\n",
    "                        weight_update.append(grad_w/len(indices))\n",
    "                        bias_update.append(grad_b/len(indices))\n",
    "                        \n",
    "                else:\n",
    "                    for j in range(len(weight_update)):\n",
    "                        (grad_w, grad_b) = grads[j]\n",
    "                        weight_update[j] = weight_update[j]+grad_w/len(indices)\n",
    "                        bias_update[j] = bias_update[j]+grad_b/len(indices)\n",
    "        \n",
    "        # Update the parameters\n",
    "        for i in range(len(weight_update)):\n",
    "            weight[i] = weight[i]-learning_rate*weight_update[-i-1]\n",
    "            bias[i] = bias[i]-learning_rate*bias_update[-i-1]\n",
    "\n",
    "        # Print the average loss\n",
    "        if epoch%10 == 0:\n",
    "            print(\"This is the %dth epoch, the average loss is%f. \"%(epoch, average_loss))\n",
    "            \n",
    "        # If performance better:\n",
    "        if average_loss<least_avg_loss:\n",
    "            #Pack and store the parameters\n",
    "            parameters = (weight,bias)\n",
    "            least_avg_loss = average_loss\n",
    "            \n",
    "    # Pack the parameters\n",
    "    parameters = (weight, bias)\n",
    "    return parameters, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2243f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a network\n",
    "def evaluate(x, y, parameters, activation_type=\"relu\"):\n",
    "    (weight, bias) = parameters\n",
    "    layers = len(weight)\n",
    "    requires_cache = False\n",
    "    \n",
    "    average_loss = 0\n",
    "    # Forward propagation\n",
    "    correct = 0\n",
    "    for i in range(len(x)):\n",
    "        output, loss, _ = neural_network(x[i], y[i], layers, weight, bias, activation_type, requires_cache)\n",
    "        average_loss += loss/len(x)\n",
    "        \n",
    "        # Take the largest index as the result\n",
    "        prediction = np.argmax(output.squeeze())\n",
    "        if y[i][prediction] == 1:\n",
    "            correct += 1\n",
    "            \n",
    "    return average_loss, correct/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab7768c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used for one-hot encoding\n",
    "def one_hot_encoder(label, class_number):\n",
    "    one_hot = np.zeros(class_number)\n",
    "    one_hot[label] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d33857ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b288aa953446ecbb9ad0e02c9b5f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d8fe531d0d44b28c486685138825d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05dfa16a3ae49459d79f80cf4e9c9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca921aa1a5b4a7caec3ba1890adcf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=transform)\n",
    "\n",
    "# Download and load the validation data\n",
    "validationset = datasets.FashionMNIST('./data', download=True, train=False, transform=transform)\n",
    "\n",
    "# Prepare the data\n",
    "train_data = []\n",
    "train_label = []\n",
    "validation_data = []\n",
    "validation_label = []\n",
    "\n",
    "for i in range(len(trainset)):\n",
    "    train_data.append(np.array([np.array(trainset[i][0].flatten())]))\n",
    "    train_label.append(one_hot_encoder(trainset[i][1], 10))\n",
    "for i in range(len(validationset)):\n",
    "    validation_data.append(np.array([np.array(validationset[i][0].flatten())]))\n",
    "    validation_label.append(one_hot_encoder(validationset[i][1], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69953245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 0th epoch, the average loss is2.302307. \n",
      "This is the 10th epoch, the average loss is2.293219. \n",
      "This is the 20th epoch, the average loss is2.284398. \n",
      "This is the 30th epoch, the average loss is2.270274. \n",
      "This is the 40th epoch, the average loss is2.236362. \n",
      "This is the 50th epoch, the average loss is2.174871. \n",
      "This is the 60th epoch, the average loss is2.129720. \n",
      "This is the 70th epoch, the average loss is2.087147. \n",
      "This is the 80th epoch, the average loss is2.025446. \n",
      "This is the 90th epoch, the average loss is1.923040. \n",
      "This is the 100th epoch, the average loss is1.784967. \n",
      "This is the 110th epoch, the average loss is1.631970. \n",
      "This is the 120th epoch, the average loss is1.458582. \n",
      "This is the 130th epoch, the average loss is1.370978. \n",
      "This is the 140th epoch, the average loss is1.240609. \n",
      "This is the 150th epoch, the average loss is1.112434. \n",
      "This is the 160th epoch, the average loss is1.053801. \n",
      "This is the 170th epoch, the average loss is0.965286. \n",
      "This is the 180th epoch, the average loss is0.885122. \n",
      "This is the 190th epoch, the average loss is0.842151. \n",
      "This is the 200th epoch, the average loss is0.800204. \n",
      "This is the 210th epoch, the average loss is0.781206. \n",
      "This is the 220th epoch, the average loss is0.756759. \n",
      "This is the 230th epoch, the average loss is0.709325. \n",
      "This is the 240th epoch, the average loss is0.685314. \n",
      "This is the 250th epoch, the average loss is0.688620. \n",
      "This is the 260th epoch, the average loss is0.660275. \n",
      "This is the 270th epoch, the average loss is0.640683. \n",
      "This is the 280th epoch, the average loss is0.645955. \n",
      "This is the 290th epoch, the average loss is0.628901. \n",
      "This is the 300th epoch, the average loss is0.646026. \n",
      "This is the 310th epoch, the average loss is0.633982. \n",
      "This is the 320th epoch, the average loss is0.622744. \n",
      "This is the 330th epoch, the average loss is0.589754. \n",
      "This is the 340th epoch, the average loss is0.602366. \n",
      "This is the 350th epoch, the average loss is0.576116. \n",
      "This is the 360th epoch, the average loss is0.595269. \n",
      "This is the 370th epoch, the average loss is0.614030. \n",
      "This is the 380th epoch, the average loss is0.550908. \n",
      "This is the 390th epoch, the average loss is0.562259. \n",
      "This is the 400th epoch, the average loss is0.575535. \n",
      "This is the 410th epoch, the average loss is0.540495. \n",
      "This is the 420th epoch, the average loss is0.528871. \n",
      "This is the 430th epoch, the average loss is0.554110. \n",
      "This is the 440th epoch, the average loss is0.566592. \n",
      "This is the 450th epoch, the average loss is0.527721. \n",
      "This is the 460th epoch, the average loss is0.532903. \n",
      "This is the 470th epoch, the average loss is0.542608. \n",
      "This is the 480th epoch, the average loss is0.555001. \n",
      "This is the 490th epoch, the average loss is0.557618. \n"
     ]
    }
   ],
   "source": [
    "structure = [len(train_data[0].squeeze()), 50, 10]\n",
    "parameters, _ = train(train_data, train_label, structure, epochs=500, learning_rate=0.1, activation_type=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3f80ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_loss, precision = evaluate(validation_data, validation_label, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f8e8e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5472080753197388 0.7989\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_loss, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906af85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
