{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec164c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ssl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8324d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# If cuda is avaliable then we use it for calculation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# This is used to handle the downloading problem\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Transform is enabled to normalize the data, here we shall construct the training& testing dataloader\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "\n",
    "# set the batch size\n",
    "batch_size = 4\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class_labels = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c08938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we design a network\n",
    "class network(nn.Module):\n",
    "    def __init__(self):    \n",
    "        super(network, self).__init__()\n",
    "        # Convolutional layers are for feature extraction\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Fully connected layers are for classification\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Max pooling\n",
    "        self.maxpooling = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dropout layer can reduce the probability of overfitting\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        # Softmax for classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        # Define the network structure\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpooling(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpooling(x)\n",
    "        \n",
    "        # Modify the size of the input\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3c2f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "model = network()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of the 0th epoch is 1.8249657\n",
      "Average loss of the 1th epoch is 1.4206435\n",
      "Average loss of the 2th epoch is 1.2560743\n",
      "Average loss of the 3th epoch is 1.1497224\n"
     ]
    }
   ],
   "source": [
    "def train(data_loader, model, epochs=20):\n",
    "    least_loss = 10000\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        \n",
    "        # for each iteration data contains samples of batch number\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # make prediction and calculate loss\n",
    "            outputs = model(inputs)\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # set the gradient to zero for updating\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # add up losses\n",
    "            avg_loss += loss.item()/len(data_loader)\n",
    "            \n",
    "        print(\"Average loss of the %dth epoch is %.7f\"%(epoch, avg_loss))\n",
    "        # if the loss is less than the least, save the model and update the least loss\n",
    "        if avg_loss < least_loss:\n",
    "            torch.save(model.state_dict(), './classifier.pth')\n",
    "            least_loss = avg_loss\n",
    "            \n",
    "    print('Finished Training')\n",
    "\n",
    "epochs = 15\n",
    "train(train_dl, model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd890040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader, model):\n",
    "    correct_predict = 0\n",
    "    avg_loss = 0\n",
    "    pred = []\n",
    "    real = []\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        # we don't require gradient when making predictions\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = data\n",
    "            real.extend(labels.tolist())\n",
    "\n",
    "            # make prediction and calculate loss\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # take the maximum as the predicted label\n",
    "            for i in range(len(outputs)):\n",
    "                pred.append(np.argmax(outputs[i]))\n",
    "                if np.argmax(outputs[i]) == labels[i]:\n",
    "                    correct_predict += 1\n",
    "\n",
    "            # add up losses\n",
    "            avg_loss += loss.item()/len(data_loader)\n",
    "    print(\"Average loss is %.7f\"%avg_loss)\n",
    "    print(\"The accuracy is %.3f\"%(correct_predict/(len(data_loader)*4)))\n",
    "    \n",
    "    # return the predictions and the real labels\n",
    "    return real, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4fcc8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss is 1.9327751\n",
      "The accuracy is 0.522\n",
      "[[640.  41.  43.   4.  25.   8.  40.  16. 137.  46.]\n",
      " [ 45. 669.  11.  11.   9.   8.  22.  20.  80. 125.]\n",
      " [115.  34. 246.  71. 187.  86. 155.  45.  32.  29.]\n",
      " [ 46.  18.  92. 225.  63. 229. 202.  51.  24.  50.]\n",
      " [ 55.  22.  89.  40. 421.  56. 177.  94.  33.  13.]\n",
      " [ 19.   9.  90. 132.  61. 454. 117.  66.  31.  21.]\n",
      " [ 11.  13.  38.  27.  65.  31. 754.  16.  14.  31.]\n",
      " [ 36.   9.  51.  37.  53.  79.  60. 608.  17.  50.]\n",
      " [158.  68.   7.   5.  16.  19.  11.   9. 664.  43.]\n",
      " [ 44. 181.  11.  28.   5.  13.  37.  38. 104. 539.]]\n"
     ]
    }
   ],
   "source": [
    "# read the model from the saved path\n",
    "model = network()\n",
    "model.load_state_dict(torch.load('./classifier.pth'))\n",
    "\n",
    "real, pred = test(test_dl, model)\n",
    "\n",
    "# construct a confusion matrix\n",
    "confusion_matrix = np.zeros([10, 10])\n",
    "for i in range(len(real)):\n",
    "    confusion_matrix[real[i]][pred[i]] += 1\n",
    "    \n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39236c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
